## 第1章 并发编程的挑战

### 1.1上下文切换

即使是单核处理器也支持多线程执行代码，CPU通过给每个线程分配CPU时间片来实现 这个机制。时间片是CPU分配给各个线程的时间，因为时间片非常短，所以CPU通过不停地切 换线程执行。

CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个 任务。但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再加载这 个任务的状态。所以任务从保存到再加载的过程就是一次上下文切换。

#### 1.1.1 多线程就一定快吗

当并发执行累加操作不超过百万次时，速度会比串行执行累加操作要慢，因为线程的创建和上下文切换都是有开销的。

#### 1.1.2 测试上下文切换次数和时长

- 使用Lmbench3 [1]可以测量上下文切换的时长
- 使用vmstat可以测量上下文切换的次数。

![](https://i.bmp.ovh/imgs/2022/05/23/4888316920f36051.png)

CS（Content Switch）表示上下文切换的次数，从上面的测试结果中我们可以看到，上下文 每1秒切换1000多次

#### 1.1.3 如何减少上下文切换

减少上下文切换的方法有无锁并发编程、CAS算法、使用最少线程和使用协程。

- 无锁并发编程

  无锁并发编程。多线程竞争锁时，会引起上下文切换，所以多线程处理数据时，可以用一 些办法来避免使用锁，如将数据的ID按照Hash算法取模分段，不同的线程处理不同段的数据。

- CAS算法

  Java的Atomic包使用CAS算法来更新数据，而不需要加锁。

- 使用最少线程

  避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样会造成大量线程都处于等待状态。

- 使用协程

  在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换

#### 1.1.4 减少上下文切换实战

### 1.2 死锁

锁是个非常有用的工具，运用场景非常多，因为它使用起来非常简单，而且易于理解。但 同时它也会带来一些困扰，那就是可能会引起死锁，一旦产生死锁，就会造成系统功能不可用。

```java
public class DeadLockDemo {
    private static String A = "A";
    private static String B = "B";
    public static void main(String[] args) {
        new DeadLockDemo().deadLock();
    }
    private void deadLock() {
        Thread t1 = new Thread(new Runnable() {
            @Override
              public void run() {
                synchronized (A) {
                    try { Thread.sleep(2000);
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                    synchronized (B) {
                        System.out.println("1");
                    }
                }
            }
        });
        Thread t2 = new Thread(new Runnable() {
            @Override
            public void run() {
                synchronized (B) {
                    synchronized (A) {
                        System.out.println("2");
                    }
                }
            }
        });
        t1.start();
        t2.start();
    }
}
```

这段代码会引起死锁，<font color="red">使线程t1和线程t2互相等待对方释放锁。</font>在复杂的业务场景中可能还会遇到这种情况：<font color="red">比如t1拿到锁之后，因为一些异常情况没有释放锁 （死循环）。</font>

一旦出现死锁，业务是可感知的，因为不能继续提供服务了，那么只能通过dump线程查看 到底是哪个线程出现了问题。

<font color="red">避免死锁的几种方法：</font>

- 避免一个线程同时获取多个锁
- 避免一个线程在锁内同时占用多个资源，尽量保证一个锁只占用一个资源
- 尝试使用<font color="red">定时锁(tryLock)</font>来代替内部锁机制
- 对于数据库锁，加锁和解锁必须在同一个数据库连接中，否则会出现解锁失败的情况

### 1.3 资源限制的挑战

- 硬件资源限制
  - 网卡带宽限制
  - 磁盘读写速度限制
  - CPU处理速度限制
- 软件资源限制
  -  数据库的连接数限制
  - socket连接数限制

### 1.4 本章小节

## 第2章 Java并发编程的底层实现原理

Java代码的底层执行过程：<font color="red">Java代码在编译后会变成Java字节码，字节码经过类加载器加载到JVM中，JVM执行字节码，最终转成成汇编指令在CPU上执行。Java中使用的并发机制依赖于JVM的实现和CPU的指令</font>

### 2.1 volatile的应用

volatile是轻量级的 synchronized，它在多处理器开发中<font color="red">保证了共享变量的“可见性”</font>。可见性的意思是当一个线程 修改一个共享变量时，另外一个线程能读到这个修改的值。
<font color="red">如果volatile变量修饰符使用恰当 的话，它比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。</font>

#### volatile的定义与实现原理

如果一个字段被声明成volatile，Java线程内存模型确保所有线程看到这个变量的值是一致的。

1. 为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存后再进行操作，但操作完不知道何时会写到内存。
2. 如果对声明了volatile的 变量进行写操作，在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一 致性协议。
3. 每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态。
4. 当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。

#### volatile的使用优化

### 2.2 synchronized的实现原理与应用

### 2.2.1 Java对象头

### 2.2.2 锁的升级和对比

### 2.3 原子操作的实现原理

### 2.4 本章小节

## 第3章 Java内存模型

### 3.1 Java内存模型基础

### 3.2 重排序

### 3.3 顺序一致性

### 3.4 volatile的内存语义

### 3.5 锁的内存语义

### 3.6 finale域的内存语义

### 3.7 happens-before原则

### 3.8 双重检查锁与延迟初始化

### 3.9 Java内存模型综述

### 3.10 本章小结









<font color="red"></font>

<font color="green"></font>
